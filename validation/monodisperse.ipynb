{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to validate MCAC after major modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Open the files for each case](#Open-the-files-for-each-case)\n",
    "    1. [Open the new case](#Open-the-new-case)\n",
    "    2. [Open the reference case](#Open-the-reference-case)\n",
    "3. [Comparison of both cases](#Comparison-of-both-cases)\n",
    "    1. [Diffusion coefficient D](#Diffusion-coefficient-D)\n",
    "    2. [Nc, L_box and N](#Nc,-L_box-and-N)\n",
    "    3. [Flow regime](#Flow-regime)\n",
    "    4. [Morphology](#Morphology)\n",
    "    5. [Aggregate polydispersity](#Aggregate-polydispersity)\n",
    "    6. [Total energy](#Total-energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Python script aims at validating MCAC by comparing with a 'well known' case already simulated in a previous version of the code (published results). This is referred as \"ref_case\" compared with the new simulation \"new_case\".\n",
    "\n",
    "In this particular code only coagulation takes place without other mechanisms of particle growth or nucleation. The aspects of the code to be validated are: the mobility (diffusion coefficient D), kinetics of aggregation (number of clusters: Nc, the size of the box: L_box, and the particle number concentration: N), flow regime (Gas Knudsen number) morpholoogy (Rg vs. Np, fractal dimension and prefactor), and aggregate polydispersity (GSD of Dv). Finally, the conservation of energy (only kinetic since interaction potential are not simulated) is tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from scipy.optimize import root_scalar, root\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from pymcac import MCAC\n",
    "from pymcac import groupby_apply, groupby_agg\n",
    "from pymcac import progress_compute, DaskDistribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Dm_equation(Dmm, f_agg):\n",
    "    Kn = 2 * lambda_g / Dmm\n",
    "    \n",
    "    cunningham = 1 + A1 * Kn + A2 * Kn * np.exp(-A3 / Kn)\n",
    "    cunningham_prime = A1 * Kn + A2 * (A3 + Kn) * np.exp(-A3 / Kn)\n",
    "    cunningham_prime2 = 2 * A1 * Kn + A2 * (A3 ** 2 / Kn + 2 * A3 + 2 * Kn) * np.exp(-A3 / Kn)\n",
    "    \n",
    "    return (\n",
    "        3 * np.pi * mu_g * Dmm - f_agg * cunningham,\n",
    "        3 * np.pi * mu_g + f_agg * cunningham_prime / Dmm,\n",
    "        - f_agg * cunningham_prime2 / Dmm ** 2\n",
    "    )\n",
    "\n",
    "dmm_norm = 1e-8\n",
    "def normalized_Dm_equation(Dmm, f_agg):\n",
    "    f_norm = 1 / f_agg / 1000\n",
    "    \n",
    "    f, df, ddf = Dm_equation(Dmm * dmm_norm, f_agg)\n",
    "    \n",
    "    return (\n",
    "        f/f_norm,\n",
    "        df / f_norm * dmm_norm,\n",
    "        ddf / f_norm * dmm_norm * dmm_norm\n",
    "    )\n",
    "\n",
    "def compute_Dm(f_agg, x0=1e-8):\n",
    "    x0 = x0 / dmm_norm\n",
    "    rootresults = root_scalar(normalized_Dm_equation, args=(f_agg,),\n",
    "                              fprime=True,  fprime2=True,\n",
    "                              x0=x0, x1=10*x0)\n",
    "    if not rootresults.converged:\n",
    "#         raise ValueError(\"Warning, compute_Dm failed to find a solution \")\n",
    "        print(\"Warning, compute_Dm failed to find a solution \")\n",
    "        return np.nan\n",
    "    return rootresults.root * dmm_norm\n",
    "\n",
    "\n",
    "def vectorized_compute_Dm(vect_f_agg, x0=1e-8):\n",
    "    res = np.empty_like(vect_f_agg)\n",
    "    for i, f_agg in enumerate(vect_f_agg):\n",
    "        res[i] = x0 = compute_Dm(f_agg, x0=x0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_missing_quantities(xAggregates, xSpheres):\n",
    "    \n",
    "    xAggregates[\"log_Rv\"] = np.log((6 * xAggregates.Volume / np.pi)) / 3\n",
    "    xAggregates[\"Ec\"] = 0.5 * xAggregates.Volume * rhop_0 * (xAggregates.lpm / xAggregates.Deltat) ** 2\n",
    "    \n",
    "    Dm = xr.apply_ufunc(vectorized_compute_Dm,\n",
    "                        xAggregates.f_agg,\n",
    "                        dask=\"parallelized\", output_dtypes=[float])\n",
    "    xAggregates[\"Kn\"] = 2 * lambda_g / Dm\n",
    "\n",
    "    xMeanRp = groupby_agg(xSpheres,\n",
    "                          by=[\"Time\", \"Label\"],\n",
    "                          lengths=(xAggregates.k.size,),\n",
    "                          sort=True,\n",
    "                          agg=[(\"Radius\", \"mean\", \"Radius\")])\n",
    "    xMeanRp = xMeanRp.chunk(xAggregates.chunks)\n",
    "    \n",
    "    xAggregates[\"DgOverDp\"] = xAggregates.Rg / xMeanRp.data\n",
    "    \n",
    "    return xAggregates, xSpheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def time_averaged_quantities(xAggregates, xSpheres, times):\n",
    "    \n",
    "    time_averaged = groupby_agg(xAggregates,\n",
    "                                by=\"Time\",\n",
    "                                set_index=False,\n",
    "                                lengths=(times.size,),\n",
    "                                sort=True,\n",
    "                                agg=[\n",
    "        (\"f_agg\", \"mean\",\"f_agg\"),\n",
    "        (\"Np\", \"mean\",\"Np\"),\n",
    "        (\"Rg\", \"mean\",\"Rg\"),\n",
    "        (\"DgOverDp\", \"mean\",\"DgOverDp\"),\n",
    "        (\"Kn\", \"mean\",\"Kn\"),\n",
    "        (\"BoxSize\", \"first\",\"BoxSize\"),\n",
    "        (\"Nc\", \"size\",\"Np\"),\n",
    "        (\"Ec\", \"sum\",\"Ec\"),\n",
    "        (\"fv\", \"sum\",\"Volume\"),\n",
    "        (\"Rv_geo\", \"mean\",\"log_Rv\"),\n",
    "        (\"sv_geo\", \"std\",\"log_Rv\")\n",
    "    ])\n",
    "    BoxVolume = time_averaged.BoxSize ** 3\n",
    "    time_averaged[\"fv\"] /= BoxVolume\n",
    "    time_averaged[\"Rv_geo\"] = np.exp(time_averaged.Rv_geo)\n",
    "    time_averaged[\"sv_geo\"] = np.exp(time_averaged.sv_geo)\n",
    "    \n",
    "    time_averaged[\"Nc density\"] = time_averaged.Nc / BoxVolume\n",
    "    time_averaged[\"Diffusion\"] = k_B * T_g / time_averaged.f_agg\n",
    "\n",
    "    Dm = xr.apply_ufunc(vectorized_compute_Dm,\n",
    "                        time_averaged.f_agg,\n",
    "                        dask=\"parallelized\", output_dtypes=[float])   \n",
    "    time_averaged[\"Kn_avg\"] = 2 * lambda_g / Dm\n",
    "    \n",
    "    xMeanRp = groupby_agg(xSpheres,\n",
    "                          by=\"Time\",\n",
    "                          set_index=False,\n",
    "                          lengths=(times.size,),\n",
    "                          sort=True,\n",
    "                          agg=[(\"Radius\", \"mean\", \"Radius\")])\n",
    "    xMeanRp = xMeanRp.chunk(time_averaged.chunks)\n",
    "    time_averaged[\"DgOverDp_avg\"] = time_averaged.Rg / xMeanRp.data\n",
    "    \n",
    "    return time_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read(path, tmax=None, ncmax=None):\n",
    "    # The folder with all .h5 and .xmf files\n",
    "    MCACSimulation = MCAC(path)\n",
    "\n",
    "    # Read all data\n",
    "    xSpheres = MCACSimulation.xspheres\n",
    "    xAggregates = MCACSimulation.xaggregates\n",
    "\n",
    "    times = MCACSimulation.times\n",
    "    \n",
    "    print(\"filtering\")\n",
    "    print(\" time\")\n",
    "    if tmax is None:\n",
    "        tmax = MCACSimulation.times[-1]\n",
    "    times = np.linspace(MCACSimulation.times[0], tmax, 100)\n",
    "    ts = np.unique((MCACSimulation.times<times[:,np.newaxis]).argmin(axis=1))\n",
    "    print(\"  xSpheres\")\n",
    "    xSpheres = xSpheres.where(xSpheres.Time.isin(MCACSimulation.times[ts]), drop=True)\n",
    "    print(\"  xAggregates\")\n",
    "    xAggregates = xAggregates.where(xAggregates.Time.isin(MCACSimulation.times[ts]), drop=True)\n",
    "\n",
    "#     if ncmax is not None:\n",
    "#         print(\" ncmax\")\n",
    "#         print(\"  xSpheres\")\n",
    "#         xSpheres = xSpheres.where(xSpheres.Label < ncmax, drop=True)\n",
    "#         print(\"  xAggregates\")\n",
    "#         xAggregates = xAggregates.where(xAggregates.Label < ncmax, drop=True)\n",
    "    print(\"done\")\n",
    "    \n",
    "    # Per aggregate computations\n",
    "    xAggregates, xSpheres = add_missing_quantities(xAggregates, xSpheres)\n",
    "    \n",
    "    # Per time-step computations\n",
    "    time_averaged = time_averaged_quantities(xAggregates, xSpheres, times)\n",
    "\n",
    "    return MCACSimulation, time_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_compare(ds1, ds2, x, y, xlabel=None, ylabel=None, **kwargs):\n",
    "    fig = plt.figure(figsize=(10, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    fig.set_tight_layout(False)\n",
    "    \n",
    "    kwargs[\"ax\"] = kwargs.get(\"ax\", plt.gca())\n",
    "\n",
    "    if x is None:\n",
    "        x = next(iter(ds1.coords))\n",
    "        \n",
    "    if isinstance(y, str):\n",
    "        y = [y]\n",
    "    \n",
    "    if isinstance(ds1, xr.DataArray):\n",
    "        ds1 = ds1.to_dataset()\n",
    "    if isinstance(ds2, xr.DataArray):\n",
    "        ds2 = ds2.to_dataset()\n",
    "\n",
    "    df1 = ds1[[x]+y].to_dataframe()\n",
    "    df2 = ds2[[x]+y].to_dataframe()\n",
    "    \n",
    "    if x in df1.columns:\n",
    "        df1 = df1.set_index(x)\n",
    "        df2 = df2.set_index(x)\n",
    "    df1 = df1.sort_index()\n",
    "    df2 = df2.sort_index()\n",
    "    \n",
    "    df1.plot(**kwargs, style=\"-\")\n",
    "    df2.plot(**kwargs, style=\"--\")\n",
    "\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    plt.legend([f\"New {varname}\" for varname in y] + [f\"Ref {varname}\" for varname in y])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PARAMETERS (check this parameters before running the script!)\n",
    "T_g = 1700  # temperature in K\n",
    "P_g = 101300  # pressure in Pa\n",
    "rhop_0 = 1800  # particle bulk density in kg/m^3\n",
    "k_B = 1.38066E-23  # Boltzmann constant in J/K\n",
    "\n",
    "# gas mean free path and viscosity\n",
    "lambda_g = 66.5E-9 * (101300 / P_g) * (T_g / 293.15) * (1 + 110 / 293.15) / (1 + 110 / T_g)  # in m\n",
    "mu_g = 18.203E-6 * (293.15 + 110) / (T_g + 110) * (T_g / 293.15) ** 1.5  # in Ps*s\n",
    "\n",
    "# for Cunningham slip correction factor: Cc\n",
    "A1 = 1.142\n",
    "A2 = 0.558\n",
    "A3 = 0.999\n",
    "\n",
    "#reference_path = Path(\"/stockage/samba/Partages/public/MCAC_validation/02_VARYING_DP/02p3_Dp10nm_np0_3200/run1\")\n",
    "reference_path = Path(\"/Data/WORK/Projets/SRC/MCAC/validation/monodisperse_data_ref/\")\n",
    "result_path = Path(\"/Data/WORK/Projets/SRC/MCAC/validation/monodisperse_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DaskDistribute().start()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Reading new\")\n",
    "newSimulation, new_time_averaged = read(result_path, tmax=0.39e-3, ncmax=100)\n",
    "\n",
    "print(\"Reading reference\")\n",
    "refSimulation, ref_time_averaged = read(reference_path, tmax=0.39e-3, ncmax=100)\n",
    "\n",
    "print(\"Start compute\")\n",
    "start = time()\n",
    "new_time_averaged, ref_time_averaged = progress_compute(new_time_averaged, ref_time_averaged)\n",
    "new_time_averaged = new_time_averaged.swap_dims({\"k\": \"Time\"})\n",
    "ref_time_averaged = ref_time_averaged.swap_dims({\"k\": \"Time\"})\n",
    "print(f\"Total Compute time : {time() - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Ref total residence time: \"\n",
    "      f\"{float(ref_time_averaged.Time[-1] - ref_time_averaged.Time[0]) * 1e3} (ms) \"\n",
    "      f\"({ref_time_averaged.Time.size} it)\")\n",
    "print(f\" New total residence time: \"\n",
    "      f\"{float(new_time_averaged.Time[-1] - new_time_averaged.Time[0]) * 1e3} (ms) \"\n",
    "      f\"({new_time_averaged.Time.size} it)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged.fv * 1e+06, ref_time_averaged.fv * 1e+06,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=\"fv\", ylabel=\"volume fraction (ppm)\",\n",
    "             loglog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"Diffusion\", ylabel=\"Diffusion coefficient (m^2/s)\",\n",
    "             loglog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=\"Nc\", ylabel=\"Number of aggregate (-)\",\n",
    "             loglog=True)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=\"BoxSize\", ylabel=\"Box size (m)\",\n",
    "             loglog=True)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=\"Nc density\", ylabel=\"Particle number concentration (-)\",\n",
    "             loglog=True)\n",
    "print(f\"Ref case initialized with {int(ref_time_averaged.Nc[0])} monomers\")\n",
    "print(f\"New case initialized with {int(new_time_averaged.Nc[0])} monomers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=\"Kn_avg\", ylabel=\"Gas Knudsen number\",\n",
    "             loglog=True)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=\"Kn\", ylabel=\"Gas Knudsen number\",\n",
    "             loglog=True)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=None, xlabel=\"Time (s)\",\n",
    "             y=[\"Kn\", \"Kn_avg\"], ylabel=\"Gas Knudsen number (computed of averages)\",\n",
    "             loglog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"Rg\", ylabel=\"Radius of gyration (m)\",\n",
    "             loglog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"DgOverDp_avg\", ylabel=\"DgOverDp\",\n",
    "             loglog=True)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"DgOverDp\", ylabel=\"DgOverDp\",\n",
    "             loglog=True)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=[\"DgOverDp\", \"DgOverDp_avg\"], ylabel=\"DgOverDp\",\n",
    "             loglog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The population-based fractal dimension and prefactor\n",
    "model = LinearRegression()\n",
    "model.fit(np.log(ref_time_averaged.DgOverDp_avg).values[:, np.newaxis],\n",
    "          np.log(ref_time_averaged.Np))\n",
    "print(f\"Ref fractal Law: {np.exp(model.intercept_)} x^{model.coef_[0]}\")\n",
    "\n",
    "# model = LinearRegression()\n",
    "model.fit(np.log(new_time_averaged.DgOverDp_avg).values[:, np.newaxis],\n",
    "          np.log(new_time_averaged.Np))\n",
    "print(f\"New fractal Law: {np.exp(model.intercept_)} x^{model.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"Rv_geo\", ylabel=\"Geometric mean vol-eq. radius, R_v (nm)\",\n",
    "             loglog=False)\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"sv_geo\", ylabel=\"Geometric standard deviation, $\\sigma_{g,rv}$ (-)\",\n",
    "             loglog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_time_averaged[\"Kinetic energy per aggregate\"] = new_time_averaged.Ec / new_time_averaged.Nc\n",
    "ref_time_averaged[\"Kinetic energy per aggregate\"] = ref_time_averaged.Ec / ref_time_averaged.Nc\n",
    "plot_compare(new_time_averaged, ref_time_averaged,\n",
    "             x=\"Np\", xlabel=\"Number of monomers per aggregate (mean)\",\n",
    "             y=\"Kinetic energy per aggregate\", ylabel=\"Kinetic energy per aggregate\",\n",
    "             loglog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
